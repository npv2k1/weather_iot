{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181960 entries, 0 to 181959\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   province  181960 non-null  object \n",
      " 1   max       181960 non-null  int64  \n",
      " 2   min       181960 non-null  int64  \n",
      " 3   wind      181960 non-null  int64  \n",
      " 4   wind_d    181960 non-null  object \n",
      " 5   rain      181960 non-null  float64\n",
      " 6   humidi    181960 non-null  int64  \n",
      " 7   cloud     181960 non-null  int64  \n",
      " 8   pressure  181960 non-null  int64  \n",
      " 9   date      181960 non-null  object \n",
      "dtypes: float64(1), int64(6), object(3)\n",
      "memory usage: 13.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Khai báo dữ liệu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing\n",
    "from xgboost import plot_importance, plot_tree\n",
    "import sqlalchemy\n",
    "import warnings\n",
    "from pmdarima.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make mysql connection\n",
    "url = \"mysql+mysqldb://root:@localhost/weather_iot\"\n",
    "engine = sqlalchemy.create_engine(url)\n",
    "\n",
    "df_sensor = pd.read_csv(\"./weather.csv\")\n",
    "df_sensor.info()\n",
    "# df_sensor.to_sql('sensor', con=engine, if_exists='replace', index=False)\n",
    "# df_sensor = pd.read_sql_table('sensor', con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lọc dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>wind</th>\n",
       "      <th>wind_d</th>\n",
       "      <th>rain</th>\n",
       "      <th>humidi</th>\n",
       "      <th>cloud</th>\n",
       "      <th>pressure</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>is_rain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-01</th>\n",
       "      <td>Ha Noi</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>NNE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70</td>\n",
       "      <td>44</td>\n",
       "      <td>1025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>Ha Noi</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>ESE</td>\n",
       "      <td>0.5</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "      <td>1025</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-03</th>\n",
       "      <td>Ha Noi</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>ESE</td>\n",
       "      <td>0.7</td>\n",
       "      <td>74</td>\n",
       "      <td>40</td>\n",
       "      <td>1022</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-04</th>\n",
       "      <td>Ha Noi</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>SSW</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81</td>\n",
       "      <td>46</td>\n",
       "      <td>1018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>Ha Noi</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>1.3</td>\n",
       "      <td>90</td>\n",
       "      <td>75</td>\n",
       "      <td>1017</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           province  max  min  wind wind_d  rain  humidi  cloud  pressure  \\\n",
       "date                                                                        \n",
       "2009-01-01   Ha Noi   19   12     6    NNE   0.0      70     44      1025   \n",
       "2009-01-02   Ha Noi   18   12     6    ESE   0.5      65     28      1025   \n",
       "2009-01-03   Ha Noi   20   13     5    ESE   0.7      74     40      1022   \n",
       "2009-01-04   Ha Noi   25   12     5    SSW   0.0      81     46      1018   \n",
       "2009-01-05   Ha Noi   23   14     5      S   1.3      90     75      1017   \n",
       "\n",
       "            day  month  year  is_rain  \n",
       "date                                   \n",
       "2009-01-01    1      1  2009        0  \n",
       "2009-01-02    2      1  2009        0  \n",
       "2009-01-03    3      1  2009        0  \n",
       "2009-01-04    4      1  2009        0  \n",
       "2009-01-05    5      1  2009        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter\n",
    "\n",
    "# Lọc dữ liệu theo tỉnh\n",
    "province_select = \"Ha Noi\"\n",
    "df_province = df_sensor[df_sensor['province'].isin([province_select])] \n",
    "\n",
    "\n",
    "# Sắp xếp lại giá trị theo thứ tự ngày\n",
    "df_province.sort_values(by=['date'])\n",
    "\n",
    "# Tách ngày, tháng, năm ra từ cột date\n",
    "df_province['day'] = df_province['date'].apply(lambda x: x.split('-')[2]).astype(int)\n",
    "df_province['month'] = df_province['date'].apply(lambda x: x.split('-')[1]).astype(int)\n",
    "df_province['year'] = df_province['date'].apply(lambda x: x.split('-')[0]).astype(int)\n",
    "df_province['is_rain'] = df_province['rain'].apply(lambda x: 1 if x > 50 else 0)\n",
    "\n",
    "# Đặt lại index là cột date\n",
    "df_province = df_province.set_index(\"date\")\n",
    "\n",
    "# Sắp xếp lại dữ liệu theo thứ tự ngày\n",
    "df_province.sort_index(inplace=True)\n",
    "df_province.head()\n",
    "df_province.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tính correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_province.corr()\n",
    "# plot correlation\n",
    "sns.heatmap(df_province.corr(), cmap='RdYlGn', annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "\n",
    "# corr = df_ha_noi.corr()\n",
    "# plt.subplots(figsize=(8, 8))\n",
    "# sns.heatmap(corr, cmap='RdYlGn', annot=True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to mysql\n",
    "df_province.corr().to_sql(name='weather_corr', con=engine, if_exists = 'replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data virtualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_province['min'].plot(figsize=(15, 6))\n",
    "df_province['max'].plot(figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rain\n",
    "df_province['rain'].plot(figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot humidity\n",
    "df_province['wind'].plot(figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw cloud\n",
    "df_province['cloud'].plot(figsize=(15, 6))\n",
    "# draw rain\n",
    "df_province['rain'].plot(figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from XGBoost import XGBoostRegressor\n",
    "label_select = 'is_rain'\n",
    "feature_select = df_province.columns.drop([label_select] + [\"wind_d\", \"province\", \"min\", \"wind\", \"cloud\", \"rain\"]).tolist()\n",
    "x_train,x_test = train_test_split(df_province, train_size=0.8)\n",
    "# train split\n",
    "X_train = x_train[feature_select]\n",
    "y_train = x_train[label_select]\n",
    "# test split\n",
    "X_test = x_test[feature_select]\n",
    "y_test = x_test[label_select]\n",
    "\n",
    "# remove index \n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "print(np.shape(X_train))\n",
    "# reshape y_train (1000,1)\n",
    "y_train = y_train.to_numpy().reshape(len(y_train), 1)\n",
    "print(np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XGBoost import XGBoostRegressor\n",
    "reg = XGBoostRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print('r2 score: ', r2_score(y_test, y_pred))\n",
    "print('rmse: ', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "# print socre\n",
    "# print(\"Score: \", r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tự code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_province.corr()\n",
    "corr = corr.drop(['min', 'max', 'wind', 'cloud', 'rain'])\n",
    "corr['max'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save SQL Error\n",
      "==========\n",
      "Feature select:  ['humidi', 'pressure', 'day', 'month', 'year']\n",
      "Label:  max\n",
      "Result:  max descision_tree 0.6411482465628269 3.609054547284472\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def auto_train(df_province,feature_select,label_select, model_name = \"xgboost\", use_lib = True):\n",
    "\n",
    "    # calc correlation\n",
    "    corr = df_province.corr()\n",
    "    # get correlation of label select\n",
    "    corr = corr[label_select]\n",
    "    # get only feature select\n",
    "    corr = corr[feature_select]\n",
    "\n",
    "    \n",
    "\n",
    "    x_train,x_test = train_test_split(df_province, train_size=0.8)\n",
    "    # train split\n",
    "    X_train = x_train[feature_select]\n",
    "    y_train = x_train[label_select]\n",
    "    # test split\n",
    "    X_test = x_test[feature_select]\n",
    "    y_test = x_test[label_select]\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "    y_train = y_train.to_numpy().reshape(len(y_train), 1)\n",
    "\n",
    "    reg = None\n",
    "    if use_lib:\n",
    "        # switch model\n",
    "        if model_name == 'xgboost':\n",
    "            reg = xgb.XGBRegressor(n_estimators=1500, booster='dart', n_jobs=8, importance_type='weight')\n",
    "            reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False) # too much logs \n",
    "\n",
    "        if model_name == \"descision_tree\":\n",
    "            from sklearn.tree import DecisionTreeRegressor\n",
    "            reg = DecisionTreeRegressor()\n",
    "            reg.fit(X_train, y_train)\n",
    "        \n",
    "        if model_name == \"random_forest\":\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            reg = RandomForestRegressor()\n",
    "            reg.fit(X_train, y_train)\n",
    "        if model_name == \"regression\":\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X_train, y_train)\n",
    "        if model_name ==\"ridge\":\n",
    "            from sklearn.linear_model import Ridge\n",
    "            reg = Ridge()\n",
    "            reg.fit(X_train, y_train)\n",
    "        if model_name ==\"svm\":\n",
    "            from sklearn.svm import SVR\n",
    "            reg = SVR()\n",
    "            reg.fit(X_train, y_train)\n",
    "        if model_name ==\"knn\":\n",
    "            from sklearn.neighbors import KNeighborsRegressor\n",
    "            reg = KNeighborsRegressor()\n",
    "            reg.fit(X_train, y_train)\n",
    "\n",
    "    else: \n",
    "        if model_name == \"descision_tree\":\n",
    "            from nguyen import DecisionTreeRegression\n",
    "            reg = DecisionTreeRegression()\n",
    "            reg.train(X_train, y_train)\n",
    "        if model_name == \"ridge\":\n",
    "            from hung import RidgeRegression\n",
    "            X_train = np.array(x_train[feature_select].values.tolist())\n",
    "            # y_train = y_train.to_numpy().reshape(len(y_train), 1)\n",
    "            # Model training\t\n",
    "            reg = RidgeRegression(lamda = 0.1, learning_rate =  1e-7, iteration = 30000)          \n",
    "            reg.train(X_train,y_train)\n",
    "            X_test = np.array(x_test[feature_select].values.tolist())\n",
    "            pred = reg.predict(X_test)\n",
    "            r2 = r2_score(y_test, pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "            error = pd.DataFrame({'r2_score': [r2_score(y_test, pred)], 'rmse': [np.sqrt(mean_squared_error(y_test, pred))]})\n",
    "       \n",
    "            # save y_test, y_pred to mysql\n",
    "            y_test = x_test[label_select]\n",
    "            \n",
    "            y_test = y_test.reset_index(drop=True)\n",
    "            y_test = y_test.to_frame()\n",
    "            y_test['pred'] = np.reshape(pred, (len(pred), 1))\n",
    "            y_test['date'] = x_test.index         \n",
    "         \n",
    "\n",
    "            try:\n",
    "               \n",
    "                error.to_sql(f'error_{label_select}', con=engine, if_exists='replace', index=False)\n",
    "                y_test.to_sql(f'test_{label_select}', con=engine, if_exists='replace', index=False)\n",
    "                \n",
    "            except:\n",
    "                print(\"Save SQL Error\")\n",
    "\n",
    "\n",
    "            return (reg,r2,rmse)\n",
    "        if model_name == \"regression\":\n",
    "            from linh import LinearRegression           \n",
    "            X_train = np.array(x_train[feature_select].values.tolist())\n",
    "            # y_train = y_train.to_numpy().reshape(len(y_train), 1)\n",
    "            reg = LinearRegression(learning_rate = 0.0000001, iteration = 30000)\n",
    "            reg.train(X_train, y_train)\n",
    "            X_test = np.array(x_test[feature_select].values.tolist())\n",
    "            pred = reg.predict(X_test)\n",
    "            r2 = r2_score(y_test, pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "            error = pd.DataFrame({'r2_score': [r2_score(y_test, pred)], 'rmse': [np.sqrt(mean_squared_error(y_test, pred))]})\n",
    "       \n",
    "            # save y_test, y_pred to mysql\n",
    "            y_test = x_test[label_select]\n",
    "            \n",
    "            y_test = y_test.reset_index(drop=True)\n",
    "            y_test = y_test.to_frame()\n",
    "            y_test['pred'] = np.reshape(pred, (len(pred), 1))\n",
    "            y_test['date'] = x_test.index\n",
    "            try:               \n",
    "                error.to_sql(f'error_{label_select}', con=engine, if_exists='replace', index=False)\n",
    "                y_test.to_sql(f'test_{label_select}', con=engine, if_exists='replace', index=False)                \n",
    "            except:\n",
    "                print(\"Save SQL Error\")\n",
    "\n",
    "\n",
    "            return (reg,r2,rmse)\n",
    "\n",
    "\n",
    "    if not reg:\n",
    "        return (None,0,0)\n",
    " \n",
    "\n",
    "    # predict\n",
    "    pred = reg.predict(X_test)\n",
    "    # plt.figure(figsize=(20, 6))\n",
    "    # reset index to plot \n",
    "\n",
    "    # plt.plot(y_test_copy, label='actual')\n",
    "    # plt.plot(pred, label='Predicted')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # reset train split\n",
    "    X_train = x_train[feature_select]\n",
    "    y_train = x_train[label_select]\n",
    "    # test split\n",
    "    X_test = x_test[feature_select]\n",
    "    y_test = x_test[label_select]\n",
    "    # error\n",
    "    # r2 and rmse\n",
    "    \n",
    "    r2 = r2_score(y_test, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "    # save error to mysql\n",
    "    error = pd.DataFrame({'r2_score': [r2_score(y_test, pred)], 'rmse': [np.sqrt(mean_squared_error(y_test, pred))]})\n",
    "    \n",
    "    # save y_test, y_pred to mysql\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    y_test = y_test.to_frame()\n",
    "    y_test['pred'] = np.reshape(pred, (len(pred), 1))\n",
    "    y_test['date'] = x_test.index   \n",
    "\n",
    "    # save y_train to mysql\n",
    "    y_train = x_train[label_select]\n",
    "    y_train = y_train.to_frame()\n",
    "    y_train['date'] = y_train.index\n",
    "\n",
    "    try:\n",
    "        corr.to_sql(name=f'corr_{label_select}', con=engine, if_exists = 'replace', index=True)\n",
    "        error.to_sql(f'error_{label_select}', con=engine, if_exists='replace', index=False)\n",
    "        y_test.to_sql(f'test_{label_select}', con=engine, if_exists='replace', index=False)\n",
    "        y_train.to_sql(f'train_{label_select}', con=engine, if_exists='replace', index=False)\n",
    "    except:\n",
    "        print(\"Save SQL Error\")\n",
    "    \n",
    "    return (reg, r2, rmse)\n",
    "\n",
    "\n",
    "list_model = [\"xgboost\", \"descision_tree\", \"random_forest\",\"regression\", \"ridge\", \"knn\",\"svm\"]\n",
    "model_name = \"descision_tree\"\n",
    "use_lib = False\n",
    "# select feature\n",
    "label_select = 'max'\n",
    "# feature_select = df_province.columns.drop([label_select] + [\"wind_d\", \"province\", \"min\", \"wind\", \"cloud\", \"rain\"]).tolist()\n",
    "feature_select = ['humidi', 'pressure', 'day', 'month', 'year']\n",
    "\n",
    "reg, r2 ,rmse = auto_train(df_province,feature_select,label_select, model_name = model_name, use_lib = use_lib)\n",
    "print(\"=\"*10)\n",
    "print(\"Feature select: \", feature_select)\n",
    "print(\"Label: \", label_select)\n",
    "print(\"Result: \", label_select, model_name, r2, rmse)\n",
    "\n",
    "# Temp min\n",
    "# label_select = 'min'\n",
    "# feature_select = ['humidi', 'pressure', 'day', 'month', 'year']\n",
    "# reg, r2 ,rmse = auto_train(df_province,feature_select,label_select, model_name = model_name, use_lib = use_lib)\n",
    "# print(\"=\"*10)\n",
    "# print(\"Feature select: \", feature_select)\n",
    "# print(\"Label: \", label_select)\n",
    "# print(\"Result: \", label_select, model_name, r2, rmse)\n",
    "\n",
    "# Rain\n",
    "# label_select = 'rain'\n",
    "# feature_select = ['humidi', 'pressure', 'min', 'max', 'wind', 'cloud']\n",
    "# reg, r2 ,rmse = auto_train(df_province,feature_select,label_select)\n",
    "# print(\"Feature select: \", feature_select)\n",
    "# print(\"Label: \", label_select)\n",
    "# print(\"Result: \", label_select, model_name, r2, rmse)\n",
    "# print(\"=\"*10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for model_name in list_model:\n",
    "#     reg, r2,rmse = auto_train(df_province,feature_select,label_select, model_name = model_name, use_lib = True)\n",
    "#     print(\"Lib\", model_name, r2, rmse)\n",
    "#     reg, r2,rmse = auto_train(df_province,feature_select,label_select, model_name = model_name, use_lib = False)\n",
    "#     print(\"no lib >>\", model_name, r2, rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ridge\"\n",
    "use_lib = False\n",
    "label_select = 'max'\n",
    "feature_select = ['humidi', 'pressure', 'day', 'month', 'year']\n",
    "reg, r2 ,rmse = auto_train(df_province,feature_select,label_select, model_name = model_name, use_lib = use_lib)\n",
    "print(\"=\"*10)\n",
    "print(\"Feature select: \", feature_select)\n",
    "print(\"Label: \", label_select)\n",
    "print(\"Result: \", label_select, model_name, r2, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to predict\n",
    "# ['humidi', 'pressure', 'day', 'month', 'year']\n",
    "def predict(model, feature_value):\n",
    "    input = np.array(feature_value).reshape(1, -1)\n",
    "    return model.predict(input)\n",
    "\n",
    "\n",
    "predict(reg, [88,1017,9,12,2022])\n",
    "# predict to now\n",
    "# date = pd.date_range(start='2020-01-01', end='2022-12-31').to_frame().reset_index(drop=True).rename(columns={0: 'date'})\n",
    "# # predict temp min\n",
    "# date['day'] = date['date'].apply(lambda x: x.day).astype(int)\n",
    "# date['month'] = date['date'].apply(lambda x: x.month).astype(int)\n",
    "# date['year'] = date['date'].apply(lambda x: x.year).astype(int)\n",
    "# date['max'] = date.apply(lambda x: predict(x['day'], x['month'], x['year'])[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "pickle.dump(reg, open(\"model.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model thư viện\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import train_test_split\n",
    "\n",
    "from XGBoost import XGBoostRegressor\n",
    "\n",
    "def auto_train(df_province,feature_select,label_select):\n",
    "    x_train,x_test = train_test_split(df_province, train_size=0.8)\n",
    "    # train split\n",
    "    X_train = x_train[feature_select]\n",
    "    y_train = x_train[label_select]\n",
    "    # test split\n",
    "    X_test = x_test[feature_select]\n",
    "    y_test = x_test[label_select]\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_train = y_train.to_numpy().reshape(len(y_train), 1)\n",
    "\n",
    "   \n",
    "\n",
    "    # fit model form xgboost library\n",
    "    reg = xgb.XGBRegressor(n_estimators=1500, booster='dart', n_jobs=8, importance_type='weight')\n",
    "    reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False) # too much logs \n",
    "\n",
    "    # fir model with my library\n",
    "    # reg = XGBoostRegressor()\n",
    "    # reg.fit(X_train, y_train) # too much logs \n",
    "\n",
    "    # use descision tree\n",
    "    # from sklearn.tree import DecisionTreeRegressor\n",
    "    # reg = DecisionTreeRegressor()\n",
    "    # reg.fit(X_train, y_train)\n",
    "\n",
    "    # use random forest\n",
    "    # from sklearn.ensemble import RandomForestRegressor\n",
    "    # reg = RandomForestRegressor()\n",
    "    # reg.fit(X_train, y_train)\n",
    "\n",
    "    # use DecisionTreeRegression from sklearn\n",
    "    # from sklearn.tree import DecisionTreeRegressor\n",
    "    # reg = DecisionTreeRegressor()\n",
    "    # reg.fit(X_train, y_train)\n",
    "\n",
    "    # predict\n",
    "    pred = reg.predict(X_test)\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    # reset index to plot \n",
    "\n",
    "    y_test_copy = y_test.copy().reset_index(drop=True)\n",
    "    plt.plot(y_test_copy, label='actual')\n",
    "\n",
    "    plt.plot(pred, label='Predicted')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # error\n",
    "    # r2 and rmse\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    print('r2 score: ', r2_score(y_test, pred))\n",
    "    print('rmse: ', np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "    # save error to mysql\n",
    "    error = pd.DataFrame({'r2_score': [r2_score(y_test, pred)], 'rmse': [np.sqrt(mean_squared_error(y_test, pred))]})\n",
    "    # error.to_sql(f'error_{label_select}', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "    # save y_test, y_pred to mysql\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    y_test = y_test.to_frame()\n",
    "    y_test['pred'] = pred\n",
    "    # print(y_test)\n",
    "    # add date\n",
    "    y_test['date'] = x_test.index\n",
    "\n",
    "    # X_test['pred'] = pred\n",
    "    print('X_test', y_test)\n",
    "   \n",
    "\n",
    "\n",
    "    # save to mysql\n",
    "    # y_test.to_sql(f'test_{label_select}', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "    # save y_train to mysql\n",
    "    # y_train = y_train.reset_index(drop=True)\n",
    "    # y_train = y_train.to_frame()\n",
    "    # y_train['date'] = y_train.index\n",
    "    # y_train.to_sql(f'train_{label_select}', con=engine, if_exists='replace', index=False)\n",
    "    \n",
    "    return reg\n",
    "\n",
    "# select feature\n",
    "label_select = 'max'\n",
    "# feature_select = df_province.columns.drop([label_select] + [\"wind_d\", \"province\", \"min\", \"wind\", \"cloud\", \"rain\"]).tolist()\n",
    "feature_select = ['humidi', 'pressure', 'day', 'month', 'year']\n",
    "# plot corr with label\n",
    "label_corr =  df_province[feature_select + [label_select]].corr()[label_select]\n",
    "# plot corr\n",
    "sns.heatmap(label_corr.to_frame(), annot=True, fmt=\".2f\")\n",
    "\n",
    "print(feature_select)\n",
    "\n",
    "reg = auto_train(df_province,feature_select,label_select)\n",
    "\n",
    "\n",
    "# label_select = 'rain'\n",
    "# feature_select = df_province.columns.drop([label_select] + [\"wind_d\", \"province\", \"year\", \"month\",\"day\"]).tolist()\n",
    "# # plot corr with label\n",
    "# label_corr =  df_province[feature_select + [label_select]].corr()[label_select]\n",
    "# # plot corr\n",
    "# sns.heatmap(label_corr.to_frame(), annot=True, fmt=\".2f\")\n",
    "\n",
    "# print(feature_select)\n",
    "\n",
    "# reg = auto_train(df_province,feature_select,label_select)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0450fcc4c21d05381ef5d84dd31a13722ed5d0b366b9705a0957c3fb34d11e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
